#syntax=aikit:test
apiVersion: v1alpha1
debug: true
runtime: cuda
backends:
  - mamba
models:
  - name: mamba-chat/config.json
    source: https://huggingface.co/havenhq/mamba-chat/raw/d343f8ade4c870d916b362746dd23821aae132dd/config.json
  - name: mamba-chat/pytorch_model.bin
    source: https://huggingface.co/havenhq/mamba-chat/resolve/d343f8ade4c870d916b362746dd23821aae132dd/pytorch_model.bin
    sha256: 6751a8c3888564a90a7f759a620e2ddfc1ab2cc3e919f2cbaf7bfc41cc5f85e7
  - name: mamba-chat/tokenizer.json
    source: https://huggingface.co/havenhq/mamba-chat/raw/d343f8ade4c870d916b362746dd23821aae132dd/tokenizer.json
  - name: mamba-chat/tokenizer_config.json
    source: https://huggingface.co/havenhq/mamba-chat/raw/d343f8ade4c870d916b362746dd23821aae132dd/tokenizer_config.json
config: |
  - name: mamba-chat
    backend: mamba
    parameters:
      model: /models/mamba-chat
    trimsuffix:
    - <|endoftext|>
    template:
      chat_message: |
        {{if eq .RoleName \"assistant\"}}<|assistant|>{{else if eq .RoleName \"system\"}}<|system|>{{else if eq .RoleName \"user\"}}<|user|>{{end}}
        {{if .Content}}{{.Content}}{{end}}
        </s>
      chat: |
        {{.Input}}
        <|assistant|>
      completion: |
        {{.Input}}
