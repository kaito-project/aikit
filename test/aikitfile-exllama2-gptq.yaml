#syntax=aikit:test
apiVersion: v1alpha1
debug: true
backends:
  - exllama2
models:
  - name: Llama-2-7B-Chat-GPTQ/model.safetensors
    source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ/resolve/main/model.safetensors
  - name: Llama-2-7B-Chat-GPTQ/tokenizer.model
    source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ/resolve/main/tokenizer.model
  - name: Llama-2-7B-Chat-GPTQ/config.json
    source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ/resolve/main/config.json
config: |
  - name: llama-2-7b-chat
    backend: exllama2
    context_size: 4096
    parameters:
      model: "Llama-2-7B-Chat-GPTQ"
      temperature: 0.2
