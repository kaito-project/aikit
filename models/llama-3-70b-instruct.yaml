#syntax=ghcr.io/sozercan/aikit:latest
apiVersion: v1alpha1
debug: true
runtime: cuda
models:
  - name: llama-3-70b-instruct
    source: https://huggingface.co/QuantFactory/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct.Q4_K_M.gguf
    sha256: "123d5e8431dd528075ccf8e026248b84279db190d7ab744be3c69b256003c929"
    promptTemplates:
      - name: instruct
        template: |
          <|start_header_id|>{{if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"user\"}}user{{end}}<|end_header_id|>{{if .Content}}{{.Content}}{{end}}<|eot_id|>s
config: |
  - name: llama-3-70b-instruct
    backend: llama
    parameters:
      model: Meta-Llama-3-70B-Instruct.Q4_K_M.gguf
    context_size: 8192
    gpu_layers: 81
    f16: true
    batch: 512
    mmap: true
    template:
      chat_message: \"instruct\"
    stopwords:
    - <|start_header_id|>
    - <|end_header_id|>
    - <|eot_id|>
    - <|reserved_special_token
    system_prompt: \"Respond to the user in a friendly and helpful tone. Generate human-like text based on the input, and use markdown when appropriate. If the user asks for something outside of your capabilities, politely tell the user what you can and can't do. Keep the response concise and relevant to the user's request.\"
