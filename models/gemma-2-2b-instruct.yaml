#syntax=ghcr.io/sozercan/aikit:latest
apiVersion: v1alpha1
debug: true
runtime: cuda
models:
  - name: gemma-2-2b-instruct
    source: https://huggingface.co/lmstudio-community/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf
    sha256: e0aee85060f168f0f2d8473d7ea41ce2f3230c1bc1374847505ea599288a7787
    promptTemplates:
      - name: chatMsg
        template: |
          <start_of_turn>{{if eq .RoleName \"assistant\" }}model{{else}}{{ .RoleName }}{{end}}
          {{ if .Content -}}
          {{.Content -}}
          {{ end -}}<end_of_turn>
      - name: chat
        template: |
          {{ .Input }}
          <start_of_turn>model
      - name: completion
        template: |
          {{ .Input }}
config: |
  - name: gemma-2-2b-instruct
    backend: llama
    parameters:
      model: gemma-2-2b-it-Q4_K_M.gguf
    context_size: 8192
    template:
      chat_message: chatMsg
      chat: chat
      completion: completion
    repeat_penalty: 1
    stopwords:
     - \"<start_of_turn>\"
     - \"<end_of_turn>\"
     - \"<|im_end|>\"
    f16: true
    mmap: true
