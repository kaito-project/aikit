# Example configuration for vLLM-style distributed inference
# This configuration mimics the vLLM example from kubernetes-sigs/lws

image:
  repository: ghcr.io/sozercan/llama3.3
  tag: "70b"
  pullPolicy: IfNotPresent

# Enable distributed inference
leaderWorkerSet:
  enabled: true
  replicas: 1
  leaderWorkerTemplate:
    size: 3  # 1 leader + 2 workers for tensor parallel
    restartPolicy: RecreateGroupOnPodRestart

  leader:
    resources:
      limits:
        nvidia.com/gpu: "8"
        memory: 80Gi
      requests:
        cpu: 4000m
        memory: 40Gi
        nvidia.com/gpu: "8"
    args:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--tensor-parallel-size"
      - "16"  # 8 GPUs per node * 2 nodes
      - "--pipeline-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.9"
    env:
      - name: CUDA_VISIBLE_DEVICES
        value: "0,1,2,3,4,5,6,7"
      - name: NCCL_DEBUG
        value: "INFO"

  worker:
    resources:
      limits:
        nvidia.com/gpu: "8"
        memory: 80Gi
      requests:
        cpu: 4000m
        memory: 40Gi
        nvidia.com/gpu: "8"
    env:
      - name: CUDA_VISIBLE_DEVICES
        value: "0,1,2,3,4,5,6,7"
      - name: NCCL_DEBUG
        value: "INFO"

# Node selection for GPU nodes
nodeSelector:
  node.kubernetes.io/instance-type: "p4d.24xlarge"

# Ensure pods are spread across nodes
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.kubernetes.io/name
          operator: In
          values:
          - aikit
      topologyKey: kubernetes.io/hostname

# GPU tolerations
tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Security settings
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  runAsGroup: 999
  runAsNonRoot: true
  runAsUser: 1000

# Service configuration
service:
  type: ClusterIP
  port: 8080

# Disable autoscaling for distributed mode
autoscaling:
  enabled: false
