image:
  repository: ghcr.io/sozercan/llama3
  tag: "8b"
  pullPolicy: IfNotPresent

replicaCount: 1

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
podAnnotations: {}
podLabels: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8080

resources:
  limits:
    cpu: 4
    memory: 4Gi
    # nvidia.com/gpu: "1"
  requests:
    cpu: 100m
    memory: 128Mi
    # nvidia.com/gpu: "1"

livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

affinity: {}

ui:
  enabled: true
  models:
  - llama-3-8b-instruct
  defaultModel: llama-3-8b-instruct
  replicaCount: 1
  type: "lobe-chat"
  image:
    repository: lobehub/lobe-chat
    pullPolicy: IfNotPresent
    tag: "v0.161.19"
  service:
    type: ClusterIP
    port: 80
  nodeSelector: {}
  tolerations: {}
  affinity: {}

  ingress:
    enabled: false
    className: ""
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: aikit.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: aikit-tls
    #    hosts:
    #      - aikit.local

  persistentVolume:
    enabled: false
    storageClass: "default"
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    claimName: ""
