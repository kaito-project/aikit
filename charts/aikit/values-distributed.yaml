# Example configuration for distributed inference using LeaderWorkerSet
# This configuration requires wrapper components for proper distributed coordination
# See WRAPPER-DESIGN.md for implementation details

image:
  repository: ghcr.io/sozercan/llama3.3
  tag: "70b"
  pullPolicy: IfNotPresent

replicaCount: 1

# Enable LeaderWorkerSet for distributed inference
leaderWorkerSet:
  enabled: true
  replicas: 1
  leaderWorkerTemplate:
    size: 3  # 1 leader + 2 workers
    restartPolicy: RecreateGroupOnPodRestart

  # Leader configuration - requires aikit-leader wrapper component
  leader:
    image:
      repository: "ghcr.io/sozercan/aikit-leader"  # Wrapper image with coordination logic
      tag: "latest"
      pullPolicy: IfNotPresent
    resources:
      limits:
        memory: 32Gi
        nvidia.com/gpu: "2"
      requests:
        cpu: 2000m
        memory: 16Gi
        nvidia.com/gpu: "2"
    command: ["/aikit-leader"]
    args:
      - "--model-path=/models"
      - "--config-file=/config.yaml"
      - "--address=0.0.0.0"
      - "--port=8080"
      - "--debug"
    env:
      - name: LWS_GROUP_SIZE
        valueFrom:
          fieldRef:
            fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/group-size']
      - name: LWS_LEADER_ADDRESS
        valueFrom:
          fieldRef:
            fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/leader-address']

  # Worker configuration - requires aikit-worker wrapper component
  worker:
    image:
      repository: "ghcr.io/sozercan/aikit-worker"  # Wrapper image with worker coordination
      tag: "latest"
      pullPolicy: IfNotPresent
    resources:
      limits:
        memory: 16Gi
        nvidia.com/gpu: "2"
      requests:
        cpu: 1000m
        memory: 8Gi
        nvidia.com/gpu: "2"
    command: ["/aikit-worker"]
    args:
      - "--leader-address=$(LWS_LEADER_ADDRESS)"
      - "--worker-port=50051"
      - "--debug"
    env:
      - name: LWS_GROUP_SIZE
        valueFrom:
          fieldRef:
            fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/group-size']

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
podAnnotations: {}
podLabels: {}

podSecurityContext:
  fsGroup: 999
  supplementalGroups:
    - 999

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false # aikit extracts backends during runtime
  runAsGroup: 999
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: ClusterIP
  port: 8080

resources:
  limits:
    memory: 32Gi
    nvidia.com/gpu: "2"
  requests:
    cpu: 2000m
    memory: 16Gi
    nvidia.com/gpu: "2"

livenessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5

readinessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 15
  periodSeconds: 5
  timeoutSeconds: 3

# Autoscaling is disabled when using LeaderWorkerSet
autoscaling:
  enabled: false

nodeSelector:
  node.kubernetes.io/instance-type: "g5.4xlarge"  # Example GPU node type

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - aikit
        topologyKey: kubernetes.io/hostname

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

rbac:
  create: true

enableRuntimeDefaultSeccompProfile: true
postInstall:
  resources: {}
  affinity: {}
  tolerations: []
  nodeSelector: {kubernetes.io/os: linux}
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsGroup: 999
    runAsNonRoot: true
    runAsUser: 1000
  labelNamespace:
    enabled: true
    image:
      repository: registry.k8s.io/kubectl
      tag: v1.33.3
      pullPolicy: IfNotPresent
      pullSecrets: []
    podSecurity: ["pod-security.kubernetes.io/audit=restricted",
      "pod-security.kubernetes.io/audit-version=latest",
      "pod-security.kubernetes.io/warn=restricted",
      "pod-security.kubernetes.io/warn-version=latest",
      "pod-security.kubernetes.io/enforce=restricted",
      "pod-security.kubernetes.io/enforce-version=v1.30"]
    extraAnnotations: {}
    extraRules: []
    priorityClassName: ""
