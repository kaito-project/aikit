"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"sidebar":[{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"link","label":"Introduction","href":"/aikit/","docId":"intro","unlisted":false},{"type":"link","label":"Quick Start","href":"/aikit/quick-start","docId":"quick-start","unlisted":false},{"type":"link","label":"Pre-made Models","href":"/aikit/premade-models","docId":"premade-models","unlisted":false},{"type":"link","label":"Demos","href":"/aikit/demo","docId":"demo","unlisted":false}],"collapsible":true},{"type":"category","label":"Features","collapsed":false,"items":[{"type":"link","label":"Creating Model Images","href":"/aikit/create-images","docId":"create-images","unlisted":false},{"type":"link","label":"Fine Tuning","href":"/aikit/fine-tune","docId":"fine-tune","unlisted":false},{"type":"link","label":"Vision","href":"/aikit/vision","docId":"vision","unlisted":false},{"type":"link","label":"GPU Acceleration","href":"/aikit/gpu","docId":"gpu","unlisted":false},{"type":"link","label":"Kubernetes Deployment","href":"/aikit/kubernetes","docId":"kubernetes","unlisted":false},{"type":"link","label":"Image Verification","href":"/aikit/cosign","docId":"cosign","unlisted":false}],"collapsible":true},{"type":"category","label":"Specifications","collapsed":false,"items":[{"type":"link","label":"Inference API Specifications","href":"/aikit/specs-inference","docId":"specs-inference","unlisted":false},{"type":"link","label":"Fine Tuning API Specifications","href":"/aikit/specs-finetune","docId":"specs-finetune","unlisted":false}],"collapsible":true},{"type":"category","label":"Inference Supported Backends","collapsed":false,"items":[{"type":"link","label":"llama.cpp (GGUF and GGML)","href":"/aikit/llama-cpp","docId":"llama-cpp","unlisted":false},{"type":"link","label":"Exllama (GPTQ)","href":"/aikit/exllama","docId":"exllama","unlisted":false},{"type":"link","label":"Exllama v2 (GPTQ and EXL2)","href":"/aikit/exllama2","docId":"exllama2","unlisted":false},{"type":"link","label":"Mamba","href":"/aikit/mamba","docId":"mamba","unlisted":false},{"type":"link","label":"Stable Diffusion","href":"/aikit/stablediffusion","docId":"stablediffusion","unlisted":false}],"collapsible":true}]},"docs":{"cosign":{"id":"cosign","title":"Image Verification","description":"AIKit and pre-made models are keyless signed with OIDC in GitHub Actions with cosign. You can verify the images with the following commands:","sidebar":"sidebar"},"create-images":{"id":"create-images","title":"Creating Model Images","description":"This section shows how to create a custom image with models of your choosing. If you want to use one of the pre-made models, skip to running models.","sidebar":"sidebar"},"demo":{"id":"demo","title":"Demos","description":"Building an image with a Llama 2 model","sidebar":"sidebar"},"exllama":{"id":"exllama","title":"Exllama (GPTQ)","description":"Exllama is a standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.","sidebar":"sidebar"},"exllama2":{"id":"exllama2","title":"Exllama v2 (GPTQ and EXL2)","description":"ExLlamaV2 is an inference library for running local LLMs on modern consumer GPUs.","sidebar":"sidebar"},"fine-tune":{"id":"fine-tune","title":"Fine Tuning","description":"Fine tuning process allows the adaptation of pre-trained models to domain-specific data. At this time, AIKit fine tuning process is only supported with NVIDIA GPUs.","sidebar":"sidebar"},"gpu":{"id":"gpu","title":"GPU Acceleration","description":"At this time, only NVIDIA GPU acceleration is supported. Please open an issue if you\'d like to see support for other GPU vendors.","sidebar":"sidebar"},"intro":{"id":"intro","title":"Introduction","description":"AIKit is a one-stop shop to quickly get started to host, deploy, build and fine-tune large language models (LLMs).","sidebar":"sidebar"},"kubernetes":{"id":"kubernetes","title":"Kubernetes Deployment","description":"It is easy to get started to deploy your models to Kubernetes!","sidebar":"sidebar"},"llama-cpp":{"id":"llama-cpp","title":"llama.cpp (GGUF and GGML)","description":"Llama.cpp is a port of Facebook\'s LLaMA model in C/C++.","sidebar":"sidebar"},"mamba":{"id":"mamba","title":"Mamba","description":"Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of Transformers.","sidebar":"sidebar"},"premade-models":{"id":"premade-models","title":"Pre-made Models","description":"AIKit comes with pre-made models that you can use out-of-the-box!","sidebar":"sidebar"},"quick-start":{"id":"quick-start","title":"Quick Start","description":"You can get started with AIKit quickly on your local machine without a GPU!","sidebar":"sidebar"},"specs-finetune":{"id":"specs-finetune","title":"Fine Tuning API Specifications","description":"v1alpha1","sidebar":"sidebar"},"specs-inference":{"id":"specs-inference","title":"Inference API Specifications","description":"v1alpha1","sidebar":"sidebar"},"stablediffusion":{"id":"stablediffusion","title":"Stable Diffusion","description":"https://github.com/EdVince/Stable-Diffusion-NCNN","sidebar":"sidebar"},"vision":{"id":"vision","title":"Vision","description":"Vision is supported through llama-cpp and llava.","sidebar":"sidebar"}}}')}}]);