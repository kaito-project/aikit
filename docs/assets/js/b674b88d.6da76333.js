"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[972],{4956:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>r});var a=t(4848),s=t(8453);const i={title:"Inference API Specifications"},o=void 0,l={id:"specs-inference",title:"Inference API Specifications",description:"v1alpha1",source:"@site/docs/specs-inference.md",sourceDirName:".",slug:"/specs-inference",permalink:"/aikit/docs/specs-inference",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/aikit/blob/main/website/docs/docs/specs-inference.md",tags:[],version:"current",frontMatter:{title:"Inference API Specifications"},sidebar:"sidebar",previous:{title:"Supply Chain Security",permalink:"/aikit/docs/security"},next:{title:"Fine Tuning API Specifications",permalink:"/aikit/docs/specs-finetune"}},c={},r=[{value:"v1alpha1",id:"v1alpha1",level:2}];function p(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"v1alpha1",children:"v1alpha1"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'apiVersion: # required. only v1alpha1 is supported at the moment\ndebug: # optional. if set to true, debug logs will be printed\nruntime: # optional. defaults to avx. can be "avx", "avx2", "avx512", "cuda"\nbackends: # optional. list of additional backends. can be "llama-cpp" (default), "exllama2", "diffusers"\nmodels: # required. list of models to build\n  - name: # required. name of the model\n    source: # required. source of the model. can be a url or a local file\n    sha256: # optional. sha256 hash of the model file\n    promptTemplates: # optional. list of prompt templates for a model\n      - name: # required. name of the template\n        template: # required. template string\nconfig: # optional. list of config files\n'})}),"\n",(0,a.jsx)(n.p,{children:"Example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'#syntax=ghcr.io/kaito-project/aikit/aikit:latest\napiVersion: v1alpha1\ndebug: true\nruntime: cuda\nmodels:\n  - name: llama-2-7b-chat\n    source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n    sha256: "08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa"\n    promptTemplates:\n      - name: "llama-2-7b-chat"\n        template: |\n          {{if eq .RoleName \\"assistant\\"}}{{.Content}}{{else}}\n          [INST]\n          {{if .SystemPrompt}}{{.SystemPrompt}}{{else if eq .RoleName \\"system\\"}}<<SYS>>{{.Content}}<</SYS>>\n\n          {{else if .Content}}{{.Content}}{{end}}\n          [/INST]\n          {{end}}\nconfig: |\n  - name: \\"llama-2-7b-chat\\"\n    backend: \\"llama\\"\n    parameters:\n      top_k: 80\n      temperature: 0.2\n      top_p: 0.7\n      model: \\"llama-2-7b-chat.Q4_K_M.gguf\\"\n    context_size: 4096\n    roles:\n      function: \'Function Result:\'\n      assistant_function_call: \'Function Call:\'\n      assistant: \'Assistant:\'\n      user: \'User:\'\n      system: \'System:\'\n    template:\n      chat_message: \\"llama-2-7b-chat\\"\n    system_prompt: \\"You are a helpful assistant, below is a conversation, please respond with the next message and do not ask follow-up questions\\"\n'})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var a=t(6540);const s={},i=a.createContext(s);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);