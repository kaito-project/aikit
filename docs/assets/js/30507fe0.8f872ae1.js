"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[341],{5772:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>l,metadata:()=>o,toc:()=>r});var a=i(5893),t=i(1151);const l={title:"Creating Model Images"},s=void 0,o={id:"create-images",title:"Creating Model Images",description:"This section shows how to create a custom image with models of your choosing. If you want to use one of the pre-made models, skip to running models.",source:"@site/docs/create-images.md",sourceDirName:".",slug:"/create-images",permalink:"/aikit/docs/create-images",draft:!1,unlisted:!1,editUrl:"https://github.com/sozercan/aikit/blob/main/website/docs/docs/create-images.md",tags:[],version:"current",frontMatter:{title:"Creating Model Images"},sidebar:"sidebar",previous:{title:"Demos",permalink:"/aikit/docs/demo"},next:{title:"Fine Tuning",permalink:"/aikit/docs/fine-tune"}},c={},r=[{value:"Quick Start",id:"quick-start",level:2},{value:"Hugging Face",id:"hugging-face",level:3},{value:"HTTP(S)",id:"https",level:3},{value:"OCI Artifacts",id:"oci-artifacts",level:3},{value:"Build Arguments",id:"build-arguments",level:3},{value:"<code>model</code>",id:"model",level:4},{value:"<code>runtime</code>",id:"runtime",level:4},{value:"Multi-Platform Support",id:"multi-platform-support",level:3},{value:"Advanced Usage",id:"advanced-usage",level:2},{value:"Running models",id:"running-models",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["This section shows how to create a custom image with models of your choosing. If you want to use one of the pre-made models, skip to ",(0,a.jsx)(n.a,{href:"#running-models",children:"running models"}),"."]})}),"\n",(0,a.jsx)(n.p,{children:"First, create a buildx buildkit instance."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker buildx create --use --name aikit-builder\n"})}),"\n",(0,a.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,a.jsx)(n.p,{children:"You can easily build an image using the following ways:"}),"\n",(0,a.jsx)(n.h3,{id:"hugging-face",children:"Hugging Face"}),"\n",(0,a.jsxs)(n.p,{children:["\ud83c\udfac Demo: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=DI5NbdEFLC8",children:"https://www.youtube.com/watch?v=DI5NbdEFLC8"})]}),"\n",(0,a.jsxs)(n.p,{children:["You can use ",(0,a.jsx)(n.a,{href:"https://huggingface.co",children:"Hugging Face"})," models directly by providing the model URL. For example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'docker buildx build -t my-model --load \\\n\t--build-arg="model=huggingface://TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf" \\\n\t"https://raw.githubusercontent.com/sozercan/aikit/main/models/aikitfile.yaml"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Resulting model name will be the file name. In this case, ",(0,a.jsx)(n.code,{children:"llama-2-7b-chat.Q4_K_M.gguf"}),"."]}),"\n",(0,a.jsxs)(n.admonition,{type:"tip",children:[(0,a.jsxs)(n.p,{children:["Syntax for Hugging Face source is ",(0,a.jsx)(n.code,{children:"huggingface://{organization}/{repository}/{branch}/{file}"}),"."]}),(0,a.jsxs)(n.p,{children:["If the branch is ",(0,a.jsx)(n.code,{children:"main"}),", it can be omitted (",(0,a.jsx)(n.code,{children:"huggingface://{organization}/{repository}/{file}"}),")."]})]}),"\n",(0,a.jsx)(n.h3,{id:"https",children:"HTTP(S)"}),"\n",(0,a.jsx)(n.p,{children:"You can use HTTP(S) URLs to download models. For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'docker buildx build -t my-model --load \\\n    --build-arg="model=https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf" \\\n    "https://raw.githubusercontent.com/sozercan/aikit/main/models/aikitfile.yaml"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Resulting model name will be the file name. In this case, ",(0,a.jsx)(n.code,{children:"llama-2-7b-chat.Q4_K_M.gguf"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"oci-artifacts",children:"OCI Artifacts"}),"\n",(0,a.jsxs)(n.p,{children:["\ud83c\udfac Demo: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=G6PrzhEe_p8",children:"https://www.youtube.com/watch?v=G6PrzhEe_p8"})]}),"\n",(0,a.jsx)(n.p,{children:"You can use OCI artifacts to download models. For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'docker buildx build -t my-model --load \\\n    --build-arg="model=oci://registry.ollama.ai/library/llama3:8b" \\\n    "https://raw.githubusercontent.com/sozercan/aikit/main/models/aikitfile.yaml"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Resulting model name will be the image name. In this case, ",(0,a.jsx)(n.code,{children:"llama3"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["After building the image, you can proceed to ",(0,a.jsx)(n.a,{href:"#running-models",children:"running models"})," to start the server."]}),"\n",(0,a.jsx)(n.h3,{id:"build-arguments",children:"Build Arguments"}),"\n",(0,a.jsx)(n.p,{children:"Below are the build arguments you can use to customize the image:"}),"\n",(0,a.jsx)(n.h4,{id:"model",children:(0,a.jsx)(n.code,{children:"model"})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"model"})," build argument is the model URL to download and use. You can use any Hugging Face (",(0,a.jsx)(n.code,{children:"huggingface://"}),"), HTTP(S) (",(0,a.jsx)(n.code,{children:"http://"})," or ",(0,a.jsx)(n.code,{children:"https://"}),"), or OCI (",(0,a.jsx)(n.code,{children:"oci://"}),"). For example:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:'--build-arg="model=huggingface://TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf"'})}),"\n",(0,a.jsx)(n.h4,{id:"runtime",children:(0,a.jsx)(n.code,{children:"runtime"})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"runtime"})," build argument adds the applicable runtimes to the image. By default, aikit will automatically choose the most optimized CPU runtime. You can use ",(0,a.jsx)(n.code,{children:"cuda"})," to include NVIDIA CUDA runtime libraries. For example:"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:'--build-arg="runtime=cuda"'}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"multi-platform-support",children:"Multi-Platform Support"}),"\n",(0,a.jsxs)(n.p,{children:["AIKit supports AMD64 and ARM64 multi-platform images. To build a multi-platform image, you can simply add ",(0,a.jsx)(n.code,{children:"--platform linux/amd64,linux/arm64"})," to the build command. For example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'docker buildx build -t my-model --load \\\n    --platform linux/amd64,linux/arm64 \\\n    --build-arg="model=huggingface://TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf" \\\n    "https://raw.githubusercontent.com/sozercan/aikit/main/models/aikitfile.yaml"\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://sozercan.github.io/aikit/docs/premade-models",children:"Pre-made models"})," are offered with multi-platform support. Docker runtime will automatically choose the correct platform to run the image. For more information, please see ",(0,a.jsx)(n.a,{href:"https://docs.docker.com/build/building/multi-platform/",children:"multi-platform images documentation"}),"."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["Please note that ARM64 support only applies to the ",(0,a.jsx)(n.code,{children:"llama.cpp"})," backend with CPU inference. NVIDIA CUDA is not supported on ARM64 at this time."]})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-usage",children:"Advanced Usage"}),"\n",(0,a.jsxs)(n.p,{children:["\ud83c\udfac Demo: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=5AQfG5VwN2c&list=PLx4Tje2rS923Bkw83GkobOyjIFLfxNrvs&index=2",children:"https://www.youtube.com/watch?v=5AQfG5VwN2c&list=PLx4Tje2rS923Bkw83GkobOyjIFLfxNrvs&index=2"})]}),"\n",(0,a.jsxs)(n.p,{children:["Create an ",(0,a.jsx)(n.code,{children:"aikitfile.yaml"})," with the following structure:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"#syntax=ghcr.io/sozercan/aikit:latest\napiVersion: v1alpha1\nmodels:\n  - name: llama-2-7b-chat.Q4_K_M.gguf\n    source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n"})}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["For full ",(0,a.jsx)(n.code,{children:"aikitfile"})," inference specifications, see ",(0,a.jsx)(n.a,{href:"/aikit/docs/specs-inference",children:"Inference API Specifications"}),"."]})}),"\n",(0,a.jsx)(n.p,{children:"Then build your image with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker buildx build . -t my-model -f aikitfile.yaml --load\n"})}),"\n",(0,a.jsx)(n.p,{children:"This will build a local container image with your model(s). You can see the image with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker images\nREPOSITORY    TAG       IMAGE ID       CREATED             SIZE\nmy-model      latest    e7b7c5a4a2cb   About an hour ago   5.51GB\n"})}),"\n",(0,a.jsx)(n.h3,{id:"running-models",children:"Running models"}),"\n",(0,a.jsx)(n.p,{children:"You can start the inferencing server for your models with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# for pre-made models, replace "my-model" with the image name\ndocker run -d --rm -p 8080:8080 my-model\n'})}),"\n",(0,a.jsxs)(n.p,{children:["You can then send requests to ",(0,a.jsx)(n.code,{children:"localhost:8080"})," to run inference from your models. For example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d \'{\n     "model": "llama-2-7b-chat.Q4_K_M.gguf",\n     "messages": [{"role": "user", "content": "explain kubernetes in a sentence"}]\n   }\'\n'})}),"\n",(0,a.jsx)(n.p,{children:"Output should be similar to:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n    "created": 1701236489,\n    "object": "chat.completion",\n    "id": "dd1ff40b-31a7-4418-9e32-42151ab6875a",\n    "model": "llama-2-7b-chat",\n    "choices": [\n        {\n            "index": 0,\n            "finish_reason": "stop",\n            "message": {\n                "role": "assistant",\n                "content": "\\nKubernetes is a container orchestration system that automates the deployment, scaling, and management of containerized applications in a microservices architecture."\n            }\n        }\n    ],\n    "usage": {\n        "prompt_tokens": 0,\n        "completion_tokens": 0,\n        "total_tokens": 0\n    }\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>o,a:()=>s});var a=i(7294);const t={},l=a.createContext(t);function s(e){const n=a.useContext(l);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(l.Provider,{value:n},e.children)}}}]);