"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"sidebar":[{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"link","label":"Introduction","href":"/aikit/docs/","docId":"intro","unlisted":false},{"type":"link","label":"Quick Start","href":"/aikit/docs/quick-start","docId":"quick-start","unlisted":false},{"type":"link","label":"Pre-made Models","href":"/aikit/docs/premade-models","docId":"premade-models","unlisted":false},{"type":"link","label":"Demos","href":"/aikit/docs/demo","docId":"demo","unlisted":false}],"collapsible":true},{"type":"category","label":"Features","collapsed":false,"items":[{"type":"link","label":"Creating Model Images","href":"/aikit/docs/create-images","docId":"create-images","unlisted":false},{"type":"link","label":"Fine Tuning","href":"/aikit/docs/fine-tune","docId":"fine-tune","unlisted":false},{"type":"link","label":"Vision","href":"/aikit/docs/vision","docId":"vision","unlisted":false},{"type":"link","label":"GPU Acceleration","href":"/aikit/docs/gpu","docId":"gpu","unlisted":false},{"type":"link","label":"Kubernetes Deployment","href":"/aikit/docs/kubernetes","docId":"kubernetes","unlisted":false},{"type":"link","label":"Supply Chain Security","href":"/aikit/docs/security","docId":"security","unlisted":false}],"collapsible":true},{"type":"category","label":"Specifications","collapsed":false,"items":[{"type":"link","label":"Inference API Specifications","href":"/aikit/docs/specs-inference","docId":"specs-inference","unlisted":false},{"type":"link","label":"Fine Tuning API Specifications","href":"/aikit/docs/specs-finetune","docId":"specs-finetune","unlisted":false}],"collapsible":true},{"type":"category","label":"Inference Supported Backends","collapsed":false,"items":[{"type":"link","label":"llama.cpp (GGUF and GGML)","href":"/aikit/docs/llama-cpp","docId":"llama-cpp","unlisted":false},{"type":"link","label":"Exllama (GPTQ)","href":"/aikit/docs/exllama","docId":"exllama","unlisted":false},{"type":"link","label":"Exllama v2 (GPTQ and EXL2)","href":"/aikit/docs/exllama2","docId":"exllama2","unlisted":false},{"type":"link","label":"Mamba","href":"/aikit/docs/mamba","docId":"mamba","unlisted":false},{"type":"link","label":"Stable Diffusion","href":"/aikit/docs/stablediffusion","docId":"stablediffusion","unlisted":false}],"collapsible":true},{"type":"category","label":"Contributing","collapsed":false,"items":[{"type":"link","label":"Release Process","href":"/aikit/docs/release","docId":"release","unlisted":false}],"collapsible":true}]},"docs":{"create-images":{"id":"create-images","title":"Creating Model Images","description":"This section shows how to create a custom image with models of your choosing. If you want to use one of the pre-made models, skip to running models.","sidebar":"sidebar"},"demo":{"id":"demo","title":"Demos","description":"Below are various demos to help you get started with AIKit.","sidebar":"sidebar"},"exllama":{"id":"exllama","title":"Exllama (GPTQ)","description":"Exllama is a standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.","sidebar":"sidebar"},"exllama2":{"id":"exllama2","title":"Exllama v2 (GPTQ and EXL2)","description":"ExLlamaV2 is an inference library for running local LLMs on modern consumer GPUs.","sidebar":"sidebar"},"fine-tune":{"id":"fine-tune","title":"Fine Tuning","description":"Fine tuning process allows the adaptation of pre-trained models to domain-specific data. At this time, AIKit fine tuning process is only supported with NVIDIA GPUs.","sidebar":"sidebar"},"gpu":{"id":"gpu","title":"GPU Acceleration","description":"At this time, only NVIDIA GPU acceleration is supported. Please open an issue if you\'d like to see support for other GPU vendors.","sidebar":"sidebar"},"intro":{"id":"intro","title":"Introduction","description":"AIKit is a comprehensive platform to quickly get started to host, deploy, build and fine-tune large language models (LLMs).","sidebar":"sidebar"},"kubernetes":{"id":"kubernetes","title":"Kubernetes Deployment","description":"It is easy to get started to deploy your models to Kubernetes! You can deploy your models with the Helm chart or manually.","sidebar":"sidebar"},"llama-cpp":{"id":"llama-cpp","title":"llama.cpp (GGUF and GGML)","description":"AIKit utilizes and depends on llama.cpp, which provides inference of Meta\'s LLaMA model (and others) in pure C/C++, for the llama backend.","sidebar":"sidebar"},"mamba":{"id":"mamba","title":"Mamba","description":"Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of Transformers.","sidebar":"sidebar"},"premade-models":{"id":"premade-models","title":"Pre-made Models","description":"AIKit comes with pre-made models that you can use out-of-the-box!","sidebar":"sidebar"},"quick-start":{"id":"quick-start","title":"Quick Start","description":"You can get started with AIKit quickly on your local machine without a GPU!","sidebar":"sidebar"},"release":{"id":"release","title":"Release Process","description":"The release process is as follows:","sidebar":"sidebar"},"security":{"id":"security","title":"Supply Chain Security","description":"AIKit is designed with security in mind. Our approach to supply chain security includes detailed tracking of software components, transparent build processes, and proactive vulnerability management. This ensures that every part of our software ecosystem remains secure and trustworthy.","sidebar":"sidebar"},"specs-finetune":{"id":"specs-finetune","title":"Fine Tuning API Specifications","description":"v1alpha1","sidebar":"sidebar"},"specs-inference":{"id":"specs-inference","title":"Inference API Specifications","description":"v1alpha1","sidebar":"sidebar"},"stablediffusion":{"id":"stablediffusion","title":"Stable Diffusion","description":"https://github.com/EdVince/Stable-Diffusion-NCNN","sidebar":"sidebar"},"vision":{"id":"vision","title":"Vision","description":"Vision is supported through llama-cpp and llava.","sidebar":"sidebar"}}}')}}]);