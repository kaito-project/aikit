---
title: Exllama (GPTQ)
---

A standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.

https://github.com/turboderp/exllama

## Example

:::warning
Please make sure to change syntax to `#syntax=ghcr.io/sozercan/aikit:latest` in the examples below.
:::

https://github.com/sozercan/aikit/blob/main/test/aikitfile-exllama.yaml
